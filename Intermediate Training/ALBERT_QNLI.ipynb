{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOHwRDz5gKXEDp056xQbICr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install -q transformers\n","!pip install -q datasets torch\n","!pip install -q torch"],"metadata":{"id":"x7nVLX-COy3U","executionInfo":{"status":"ok","timestamp":1742435431197,"user_tz":240,"elapsed":88689,"user":{"displayName":"Andrew Wright","userId":"06533282160522110993"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","from datasets import load_dataset\n","from transformers import AlbertForSequenceClassification\n","from transformers import AlbertTokenizer\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","from sklearn.metrics import accuracy_score\n","from transformers.utils import logging"],"metadata":{"id":"SRHCEioKOzL1","executionInfo":{"status":"ok","timestamp":1742435718659,"user_tz":240,"elapsed":2,"user":{"displayName":"Andrew Wright","userId":"06533282160522110993"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"ZIfXgXmfgtbz","executionInfo":{"status":"ok","timestamp":1742442698583,"user_tz":240,"elapsed":6952381,"user":{"displayName":"Andrew Wright","userId":"06533282160522110993"}},"outputId":"e40f75e1-4c32-4be4-9b9e-19513d109408"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-9-b9259a69e588>:44: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='19641' max='19641' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [19641/19641 1:55:01, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.512800</td>\n","      <td>0.483053</td>\n","      <td>0.782537</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.428400</td>\n","      <td>0.416606</td>\n","      <td>0.815669</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.379500</td>\n","      <td>0.382413</td>\n","      <td>0.832876</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='342' max='342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [342/342 00:42]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.8329\n"]},{"output_type":"execute_result","data":{"text/plain":["('./albert_qnli_finetuned/tokenizer_config.json',\n"," './albert_qnli_finetuned/special_tokens_map.json',\n"," './albert_qnli_finetuned/spiece.model',\n"," './albert_qnli_finetuned/added_tokens.json')"]},"metadata":{},"execution_count":9}],"source":["# use colab T4 GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# load GLUE QNLI dataset and ALBERT tokenizer\n","dataset = load_dataset(\"glue\", \"qnli\")\n","\n","tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n","\n","# tokenize dataset\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"question\"], examples[\"sentence\"],\n","                     padding=\"max_length\", truncation=True, max_length=128)\n","\n","encoded_dataset = dataset.map(preprocess_function, batched=True)\n","\n","# label mapping - 0 = \"not entailment\", 1 = \"entailment\"\n","encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n","encoded_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","\n","model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=2).to(device)\n","\n","# training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./albert_qnli\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    logging_dir=\"./logs\",\n","    logging_steps=500,\n","    load_best_model_at_end=True,\n","    report_to=\"none\"\n",")\n","\n","# evaluation function\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n","    acc = accuracy_score(labels, predictions)\n","    return {\"accuracy\": acc}\n","\n","# trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=encoded_dataset[\"train\"],\n","    eval_dataset=encoded_dataset[\"validation\"],\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","logging.set_verbosity_error()\n","\n","trainer.train()\n","\n","# model accuracy evaluation\n","eval_results = trainer.evaluate()\n","print(f\"Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n","\n","model.save_pretrained(\"./albert_qnli_finetuned\")\n","tokenizer.save_pretrained(\"./albert_qnli_finetuned\")\n"]}]}